{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda today:\n",
    "1. Overview of NLP\n",
    "2. Model Building Remains Consistent\n",
    "2. Pre-Processing for NLP \n",
    "    - Tokenization\n",
    "    - Stopwords removal\n",
    "    - Lexicon normalization: lemmatization and stemming\n",
    "3. Feature Engineering for NLP\n",
    "    - Bag-of-Words\n",
    "    - Count Vectorizer\n",
    "    - Term frequency-Inverse Document Frequency (tf-idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.student_caller import one_random_student\n",
    "from src.student_list import student_first_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Name 3 algorithms whose effectiveness depends on scaling\"\n",
    "one_random_student(student_first_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is always a good idea\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# This is always a good idea\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.student_caller import one_random_student\n",
    "from src.student_list import student_first_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of NLP\n",
    "NLP allows computers to interact with text data in a structured and sensible way.  We will be breaking up series of texts into individual words (or groups of words), and isolating the words with **semantic value**.  We will then compare texts with similar distributions of these words, and group them together.\n",
    "\n",
    "In this section, we will discuss some steps and approaches to common data analytic procedures for text to learn how computers can understand human language, its meaning and sentiments. Some of the applications of natural language processing are:\n",
    "\n",
    "- Chatbots \n",
    "- Speech recognition and audio processing \n",
    "- Classifying documents \n",
    "\n",
    "Here is an example that uses some of the tools we use in this notebook.  \n",
    "  -[chi_justice_project](https://chicagojustice.org/research/justice-media-project/)  \n",
    "  -[chicago_justice classifier](https://github.com/chicago-justice-project/article-tagging/blob/master/lib/notebooks/bag-of-words-count-stemmed-binary.ipynb)\n",
    "\n",
    "We will introduce you to the preprocessing steps, feature engineering, and other steps you need to take in order to format text data for machine learning tasks. \n",
    "\n",
    "We will also introduce you to [**NLTK**](https://www.nltk.org/) (Natural Language Toolkit), which will be our main tool for engaging with textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP process \n",
    "<img src=\"img/nlp_process.png\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Building Principles Remain Consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now been introduced to a suite of concepts concerning how to transform data of different raw forms into forms that computers can learn from. We have learned how to scale, one hot encode, bin, and resample continuous and categorical independent features for regression and classification tasks.  We have learned how to arrange autocorrelated series chronologically to predict with past value.  And now we will learn to break up text into numerical dataframes that reflect semantic value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "# conda install -c anaconda nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with a dataset which includes both **satirical** (The Onion) and serious news (Reuters) articles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab\n",
    "> We refer to the entire set of articles as the **corpus**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import our corpus. \n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "import pandas as pd\n",
    "\n",
    "corpus = pd.read_csv('data/satire_nosatire.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to detect satire, so our target class of 1 is associated with The Onion articles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![the_onion](img/the_onion.jpeg) ![reuters](img/reuters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a highly relavent task.  If we could separate real news from fictitious news, we would be able to potentially flag the latter as such.  This does come with risks.  If we deploy a model which mislabel real news as ficticious news, we will open ourselves to all sorts of criticism.  A false positive in this sense to bury a story or do damage to a reporter's reputation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Vocab\n",
    "> Each article in the corpus is refered to as a **document**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many documents are there in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the class balance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# It is a balanced dataset with 500 documents of each category. \n",
    "corpus.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some example texts from both categories.\n",
    "\n",
    "# Category 1\n",
    "corpus[corpus.target==1].sample(random_state=2).body.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category 0\n",
    "corpus[corpus.target==0].sample(random_state=1).body.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about our types of error and the use cases of being able to correctly separate satirical from authentic news. What type of error should we decide to optimize our models for?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help me fill in the blanks:\n",
    "\n",
    "# Pass in the body of the documents as raw text\n",
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "X = corpus.body\n",
    "y = corpus.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration purposes, we will perform a secondary train test split. In practice, we will apply a pipeline.\n",
    "\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train, random_state=42, test_size=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# A new preprocessing tool!\n",
    "tfidf = TfidfVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\")\n",
    "\n",
    "# Like always, we are fitting only on the training set\n",
    "X_t_vec = None\n",
    "\n",
    "# Here is our new dataframe of predictors\n",
    "X_t_vec = pd.DataFrame(X_t_vec.toarray(), columns = tfidf.get_feature_names())\n",
    "\n",
    "X_t_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# A new preprocessing tool!\n",
    "\n",
    "# This tool removes case punctuation, numbers, and stopwords in one fell swoop.\n",
    "# We will break this down below\n",
    "tfidf = TfidfVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=stopwords.words('english'))\n",
    "\n",
    "# Like always, we are fitting only on the training set\n",
    "X_t_vec = tfidf.fit_transform(X_t)\n",
    "# Here is our new dataframe of predictors\n",
    "X_t_vec = pd.DataFrame(X_t_vec.toarray(), columns = tfidf.get_feature_names())\n",
    "X_t_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's a lot of columns\n",
    "X_t_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can push this data into any of our classification models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_r = LogisticRegression()\n",
    "log_r.fit(X_t_vec, y_t)\n",
    "\n",
    "# Perfect on the training set. \n",
    "log_r.score(X_t_vec, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We proceed as usual:\n",
    "    - Transform the validation set\n",
    "    - Score the validation set\n",
    "    - Plot a confusion matrix to check out the performance on different types of errors\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# Transform X_val\n",
    "X_val_vec = None\n",
    "\n",
    "# score val\n",
    "print()\n",
    "\n",
    "# Plot confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# Transform X_val\n",
    "X_val_vec = tfidf.transform(X_val)\n",
    "y_hat = log_r.predict(X_val_vec)\n",
    "# Score val\n",
    "\n",
    "print(log_r.score(X_val_vec, y_val))\n",
    "\n",
    "# Plot confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "# we should have a lot of true positive and true negatives\n",
    "plot_confusion_matrix(log_r, X_val_vec, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did your model do?  \n",
    "\n",
    "Probably well.  \n",
    "It's pretty amazing the patterns, even a relatively low power one such as logistic regression, can find in text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "tfidf = TfidfVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\")\n",
    "lr = LogisticRegression()\n",
    "pipeline = make_pipeline(tfidf, lr)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "cross_val_score(pipeline, X_train, y_train).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "\n",
    "pipeline.score(X_test, y_test)\n",
    "y_hat = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(pipeline, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization \n",
    "\n",
    "In order to convert the texts into data suitable for machine learning, we need to break down the documents into smaller parts. \n",
    "\n",
    "The first step in doing that is **tokenization**.\n",
    "\n",
    "Tokenization is the process of splitting documents into units of observations. We usually represent the tokens as __n-gram__, where n represent the consecutive words occuring in a document. In the case of unigram (one word token), the sentence \"David works here\" can be tokenized into?\n",
    "\n",
    "\"David\", \"works\", \"here\"\n",
    "\"David works\", \"works here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the first document in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_document = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "first_document = corpus.iloc[0].body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to tokenize our document. \n",
    "\n",
    "It is a long string, so the first way we might consider is to split it by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "first_document.split()[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are attempting to create a list of tokens whose semantic value reflects the class the document is associated with.  What issues might you see with the list of tokens generated by a simple split as performed above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_random_student(student_first_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat out some problems (don't look down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/ZaiC2DYDRiqhQ269nz/giphy.gif\" style=\"width:1500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to create a set of tokens with **high semantic value**.  In other words, we want to isolate text which best represents the meaning in each document.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common text cleaning tasks:  \n",
    "  1. remove capitalization  \n",
    "  2. remove punctuation  \n",
    "  3. remove stopwords  \n",
    "  4. remove numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could manually perform all of these tasks with string operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capitalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create our matrix of words associated with our corpus, **capital letters** will mess things up.  The semantic value of a word used at the beginning of a sentence is the same as that same word in the middle of the sentence.  In the two sentences:\n",
    "\n",
    "sentence_one =  \"Excessive gerrymandering in small counties suppresses turnout.\"   \n",
    "sentence_two =  \"Turnout is suppressed in small counties by excessive gerrymandering.\"  \n",
    "\n",
    "Excessive has the same semantic value, but will be treated as two separate tokens because of capitals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fill in the list comprehension below to manually and remove capitals from the 1st document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_cleanup = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "## Manual removal of capitals\n",
    "\n",
    "manual_cleanup = [token.lower() for token in first_document.split(' ')]\n",
    "manual_cleanup[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Our initial token set for our first document is {len(manual_cleanup)} words long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Our initial token set for our first document has {len(set(first_document.split()))} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"After remove caps, our first document has {len(set(manual_cleanup))} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing capitals, we decrease the total unique word count in our first document by 2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like capitals, splitting on white space will create tokens which include punctuation that will muck up our semantics.  \n",
    "\n",
    "Returning to the above example, 'gerrymandering' and 'gerrymandering.' will be treated as different tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different ways to strip punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip with translate\n",
    "\n",
    "## Manual removal of punctuation\n",
    "# string library!\n",
    "import string\n",
    "\n",
    "string.punctuation\n",
    "punctuation = string.punctuation + '“'\n",
    "punctuation\n",
    "# string.ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punc = [word.translate(word.maketrans(\"\",\"\", punctuation)) for word in manual_cleanup]\n",
    "\n",
    "no_punc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strip with regex\n",
    "\n",
    "To remove them, we will use regular expressions, a powerful tool which you may already have some familiarity with.\n",
    "\n",
    "Regex allows us to match strings based on a pattern.  This pattern comes from a language of identifiers, which we can begin exploring on the cheatsheet found here:\n",
    "  -   https://regexr.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few key symbols:\n",
    "  - . : matches any character\n",
    "  - \\d, \\w, \\s : represent digit, word, whitespace  \n",
    "  - *, ?, +: matches 0 or more, 0 or 1, 1 or more of the preceding character  \n",
    "  - [A-Z]: matches any capital letter  \n",
    "  - [a-z]: matches lowercase letter  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other helpful resources:\n",
    "\n",
    "https://regexcrossword.com/  \n",
    "https://www.regular-expressions.info/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Test out a word and search it\n",
    "\n",
    "# Create a pattern that matches only letters to strip punctuation\n",
    "\n",
    "\n",
    "# Use re.search to search for a word.\n",
    "target_word = manual_cleanup[10]\n",
    "\n",
    "# Use the .group() method to return the matched word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# a raw string will not process escapes\n",
    "pattern = r\"[a-zA-Z]+\"\n",
    "\n",
    "target_word = manual_cleanup[10]\n",
    "re.search(pattern, target_word).group(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a list comprehension to search each word in the word list and return the match.\n",
    "# Use an if statement to make sure the .group() method does not throw an error.\n",
    "\n",
    "# Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__\n",
    "manual_cleanup = [re.search(pattern, word).group(0) for word in manual_cleanup if re.search(pattern, word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "\n",
    "# Better solution \n",
    "import re\n",
    "\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "[re.match(pattern, word).group(0) for word in manual_cleanup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"After removing punctuation, our first document has {len(set(manual_cleanup))} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are the **filler** words in a language: prepositions, articles, conjunctions. They have low semantic value, and almost always need to be removed.  \n",
    "\n",
    "Luckily, NLTK has lists of stopwords ready for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]\n",
    "\n",
    "# Try other langauges below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you notice above, stopwords come from the [nltk corpus package](http://www.nltk.org/book_1ed/ch02.html#tab-corpora).  The corpus package contains a variety of texts that are free to download and use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which stopwords are present in our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = [token for token in manual_cleanup if token in stopwords.words('english')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = [token for token in manual_cleanup if token in stopwords.words('english')]\n",
    "stops[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(stops)} stopwords in the first document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'That is {len(stops)/len(manual_cleanup): .2%} of our text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use the **FreqDist** tool to look at the makeup of our text before and after removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "fdist = FreqDist(manual_cleanup)\n",
    "plt.figure(figsize=(10,10))\n",
    "fdist.plot(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_cleanup = [token for token in manual_cleanup if token not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also customize our stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# We can also customize our stopwords list\n",
    "\n",
    "custom_sw = stopwords.words('english')\n",
    "custom_sw.extend([\"i'd\",\"say\"] )\n",
    "custom_sw[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_cleanup = [token for token in manual_cleanup if token not in custom_sw]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'After removing stopwords, there are {len(set(manual_cleanup))} unique words left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(manual_cleanup)\n",
    "plt.figure(figsize=(10,10))\n",
    "fdist.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the Frequency Dist plot, there are perhaps some more stopwords we can remove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "custom_sw.extend(['could', 'one'])\n",
    "manual_cleanup = [token for token in manual_cleanup if token not in custom_sw]\n",
    "\n",
    "fdist = FreqDist(manual_cleanup)\n",
    "plt.figure(figsize=(10,10))\n",
    "fdist.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numbers\n",
    "\n",
    "Numbers also usually have low semantic value. Their removal can help improve our models. \n",
    "\n",
    "To remove them, we will use regular expressions, a powerful tool which you may already have some familiarity with.\n",
    "\n",
    "Bringing back our regex symbols, we can quickly figure out patterns which will filter out numeric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few key symbols:\n",
    "  - . : matches any character\n",
    "  - \\d, \\w, \\s : represent digit, word, whitespace  \n",
    "  - *, ?, +: matches 0 or more, 0 or 1, 1 or more of the preceding character  \n",
    "  - [A-Z]: matches any capital letter  \n",
    "  - [a-z]: matches lowercase letter  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern to remove numbers (probably same as above)\n",
    "\n",
    "no_num_pattern = None\n",
    "\n",
    "test_string = \"Reno 911\"\n",
    "\n",
    "re.search(no_num_pattern, test_string).group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "no_num_pattern = r'[a-zA-Z]*'\n",
    "test_string = \"Reno 911\"\n",
    "\n",
    "re.search(no_num_pattern, test_string).group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair Exercise:  \n",
    "Sklearn and NLTK provide us with a suite of **tokenizers** for our text preprocessing convenience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use them to condense the steps above.  As we will see, even more steps will be condensed in the vectorizors introduced at the end of the notebook.  \n",
    "\n",
    "For now, we will still need to make our list lowercase and remove stopwords by hand. \n",
    "\n",
    "It is important to get used to the process of tokenizing by hand, since it will give us more freedom in certain preprocessing steps (see stemmers/lemmers below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, take the sample_doc below and use the tokenizer of your choice to create word tokens.  If you use the regexp vectorizor, I have included a regex pattern that does not exclude contractions.  Feel free to pass that in as an argument.\n",
    "\n",
    "After tokenizing, make tokens lowercase, and remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = X_t.sample(random_state=42).values[0]\n",
    "sample_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall(r\"([a-zA-Z]+(?:'[a-z]+)?)\" , \"I'd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# Use the tokenizer of your choice using the .tokenize() method.\n",
    "tokenizer = None\n",
    "\n",
    "# make lowercase\n",
    "\n",
    "# remove stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"([a-zA-Z]+(?:[’'][a-z]+)?)\")\n",
    "\n",
    "sample_doc = X_t.sample(random_state=42).values[0]\n",
    "sample_doc = tokenizer.tokenize(sample_doc)\n",
    "\n",
    "sample_doc = [token.lower() for token in sample_doc]\n",
    "sample_doc = [token for token in sample_doc if token not in custom_sw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the semantic meaning of a word is held in the root, which is usually the beginning of a word.  Conjugations and plurality do not change the semantic meaning. \"eat\", \"eats\", and \"eating\" all have essentially the same meaning packed into eat.   \n",
    "\n",
    "Stemmers consolidate similar words by chopping off the ends of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stemmer](img/stemmer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different stemmers available.  The two we will use here are the **Porter** and **Snowball** stemmers.  A main difference between the two is how agressively it stems, Porter being less agressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import *\n",
    "\n",
    "# instantiate a PorterStemmer and a SnowballStemmer.  Pass language='english' as an argument to the Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "p_stemmer = PorterStemmer()\n",
    "s_stemmer = SnowballStemmer(language=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the .stem() method, try out stemming some words from the sample doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "p_stemmer.stem(sample_doc[0])\n",
    "s_stemmer.stem(sample_doc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_count = 0\n",
    "for word in sample_doc:\n",
    "    p_word = p_stemmer.stem(word)\n",
    "    s_word = s_stemmer.stem(word)\n",
    "    \n",
    "    if p_word != s_word:\n",
    "        print(p_word, s_word)\n",
    "        difference_count += 1\n",
    "        \n",
    "print(\"\\n\" f\"Of the {len(sample_doc)} words in the sample doc, only {difference_count} are stemmed differently\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the snowball stemmer, and stem all the words in the doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "sample_doc = [s_stemmer.stem(word) for word in sample_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(sample_doc)\n",
    "plt.figure(figsize=(10,10))\n",
    "fdist.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Stemming reduced our token count {len(set(sample_doc))} to unique tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemming is a bit more sophisticated that the stem choppers.  Lemming uses part of speech tagging to determine how to transform a word.  In that \n",
    "Lemmatization returns real words. For example, instead of returning \"movi\" like Porter stemmer would, \"movie\" will be returned by the lemmatizer.\n",
    "\n",
    "- Unlike Stemming, Lemmatization reduces the inflected words properly ensuring that the root word belongs to the language.  It can handle words such as \"mouse\", whose plural \"mice\" the stemmers would not lump together with the original. \n",
    "\n",
    "- In Lemmatization, the root word is called Lemma. \n",
    "\n",
    "- A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n",
    "\n",
    "![lemmer](img/lemmer.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "# instantiate a lemmatizer\n",
    "lemmatizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# think of a noun with an irregular plural form and pass the string as an argument to the lemmatizer\n",
    "print(f'<Irregular noun> becomes: {lemmatizer.lemmatize()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "print(f'Mice becomes: {lemmatizer.lemmatize(\"mice\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = X_t.sample().values[0]\n",
    "tokenizer = RegexpTokenizer(r\"([a-zA-Z]+(?:[’'][a-z]+)?)\")\n",
    "\n",
    "new_sample = tokenizer.tokenize(new_sample)\n",
    "new_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{new_sample[8]} becomes: {lemmatizer.lemmatize(new_sample[8])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizers depend on POS tagging, and defaults to noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little bit of work, we can POS tag our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "tokenizer = RegexpTokenizer(r\"([a-zA-Z]+(?:[’'][a-z]+)?)\")\n",
    "new_sample = [token for token in new_sample if token not in custom_sw]\n",
    "new_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "# Use nltk's pos_tag to tag our words\n",
    "new_sample_tagged = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "new_sample_tagged = pos_tag(new_sample)\n",
    "new_sample_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then transform the tags into the tags of our lemmatizers\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_tagged = [(token[0], get_wordnet_pos(token[1]))\n",
    "             for token in new_sample_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lemmatize with the POS fed into the lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "new_sample_lemmed = [lemmatizer.lemmatize(token[0], token[1]) for token in new_sample_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_lemmed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(set(new_sample_lemmed))} unique lemmas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(new_sample_lemmed)\n",
    "plt.figure(figsize=(10,10))\n",
    "fdist.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering for NLP \n",
    "In order to pass text data to machine learning algorithm and perform classification, we need to represent the features in a sensible way. One such method is called **Bag-of-words (BoW)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bag-of-words model, or BoW for short, is a way of extracting **features** from text for use in modeling. A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "- A vocabulary of known words.\n",
    "- A measure of the presence of known words.\n",
    "\n",
    "It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether words occur in the document, **not where** in the document. The intuition behind BoW is that a document is similar to another if they have similar contents. Bag of Words method can be represented as **Document Term Matrix**, or Term Document Matrix, in which each column is an unique vocabulary, each observation is a document. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document 1: \"I love dogs\"\n",
    "- Document 2: \"I love cats\"\n",
    "- Document 3: \"I love all animals\"\n",
    "- Document 4: \"I hate dogs\"\n",
    "\n",
    "\n",
    "Can be represented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![document term matrix](img/document_term_matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing it in python\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# instantiate a count vectorizer\n",
    "vec = None\n",
    "\n",
    "# fit vectorizor on our lemmed sample. Note, the vectorizer takes in raw texts, so we need to join all of our lemmed tokens.\n",
    "X = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform([\" \".join(new_sample_lemmed)])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not very exciting for one document. The idea is to make a document term matrix for all of the words in our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass in arguments such as a regex pattern, a list of stopwords, and an ngram range to do our preprocessing in one fell swoop.   \n",
    "*Note lowercase defaults to true.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in the regex from above, our custom stopwords, and an ngram range: [1,1], [1,2] , [1,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=custom_sw, ngram_range=[1,2])\n",
    "X = vec.fit_transform(X_t)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our document term matrix gets bigger and bigger, with more and more zeros, becoming sparser and sparser.\n",
    "\n",
    "> In case you forgot, a sparse matrix \"is a matrix in which most of the elements are zero.\" [wikipedia](https://en.wikipedia.org/wiki/Sparse_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set upper and lower limits to the word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=custom_sw, ngram_range=[1,2], min_df=2, max_df=25)\n",
    "X = vec.fit_transform(corpus.body)\n",
    "\n",
    "df_cv = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF \n",
    "There are many schemas for determining the values of each entry in a document term matrix, and one of the most common schema is called the TF-IDF -- term frequency-inverse document frequency. Essentially, tf-idf *normalizes* the raw count of the document term matrix. And it represents how important a word is in the given document. \n",
    "\n",
    "> The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n",
    "\n",
    "- TF (Term Frequency)\n",
    "term frequency is the frequency of the word in the document divided by the total words in the document.\n",
    "\n",
    "- IDF (inverse document frequency)\n",
    "IDF represents the measure of how much information the word provides, i.e., if it's common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient):\n",
    "\n",
    "$$idf(w) = log (\\frac{number\\ of\\ documents}{num\\ of\\ documents\\ containing\\ w})$$\n",
    "\n",
    "tf-idf is the product of term frequency and inverse document frequency, or tf * idf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "tf_vec = TfidfVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=custom_sw)\n",
    "X = tf_vec.fit_transform(corpus.body)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns = tf_vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.iloc[313].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[313].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the tfidf to the count vectorizer output for one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=custom_sw)\n",
    "X = vec.fit_transform(corpus.body)\n",
    "\n",
    "df_cv = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv.iloc[313].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tfidf lessoned the importance of some of the more common words, including a stopword which \"also\" which didn't make it into the stopword list.\n",
    "\n",
    "It also assigns \"nerds\" more weight than power.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Nerds only shows up in document 313: {len(df_cv[df.nerds!=0])}')\n",
    "print(f'Power shows up in {len(df_cv[df.power!=0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair: \n",
    "\n",
    "For a final exercise, work through in pairs the following exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a document term matrix of the 1000 document corpus.  The vocabulary should have no stopwords, no numbers, no punctuation, and be lemmatized.  The Document-Term Matrix should be created using tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "corpus = pd.read_csv('data/satire_nosatire.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "def doc_preparer(doc, stop_words=custom_sw):\n",
    "    '''\n",
    "    \n",
    "    :param doc: a document from the satire corpus \n",
    "    :return: a document string with words which have been \n",
    "            lemmatized, \n",
    "            parsed for stopwords, \n",
    "            made lowercase,\n",
    "            and stripped of punctuation and numbers.\n",
    "    '''\n",
    "    \n",
    "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:’[a-z]+)?)\")\n",
    "    doc = regex_token.tokenize(doc)\n",
    "    doc = [word.lower() for word in doc]\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(word[0], get_wordnet_pos(word[1])) for word in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    return ' '.join(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "docs = [doc_preparer(doc) for doc in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "tf_idf = TfidfVectorizer(min_df = 3)\n",
    "X = tf_idf.fit_transform(docs)\n",
    "\n",
    "df = pd.DataFrame(X.toarray())\n",
    "df.columns = tf_idf.vocabulary_\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn_env_2)",
   "language": "python",
   "name": "learn_env_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
